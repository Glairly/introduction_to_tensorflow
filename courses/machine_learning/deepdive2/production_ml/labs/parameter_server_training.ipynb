{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parameter Server Training\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Instantiate a ParameterServerStrategy\n",
    "2. Training with Model.fit\n",
    "3. Training with Custom Training Loop\n",
    "4. Define and run an evaluation loop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "[Parameter server training](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf)\n",
    "is a common data-parallel method to scale up model training on multiple\n",
    "machines. A parameter server training cluster consists of workers and parameter\n",
    "servers. Variables are created on parameter servers and they are read and updated by workers in each step. By default, workers read and update these variables independently without synchronizing with each other. This is why sometimes parameter server-style training is called asynchronous training.\n",
    "\n",
    "In TF2, parameter server training is powered by the\n",
    "`tf.distribute.experimental.ParameterServerStrategy` class, which distributes\n",
    "the training steps to a cluster that scales up to thousands of workers\n",
    "(accompanied by parameter servers). There are two main supported training APIs:\n",
    "Keras Training API, also known as `Model.fit`, and Custom Training Loop (CTL).\n",
    "`Model.fit` is recommended when users prefer a high-level abstraction\n",
    "and handling of training, while CTL is recommended when users prefer to define the details of their training\n",
    "loop.\n",
    "\n",
    "Regardless of the API of choice, distributed training in TF2 involves a\n",
    "\"cluster\" with several \"jobs\", and each of the jobs may have one or more\n",
    "\"tasks\". When using parameter server training, it is recommended to have one\n",
    "coordinator job (which has the job name `chief`), multiple worker jobs (job name\n",
    "`worker`), and multiple parameter server jobs (job name `ps`).\n",
    "\n",
    "While the coordinator creates resources, dispatches training tasks, writes\n",
    "checkpoints, and deals with task failures, workers and parameter servers run `tf.distribute.Server` that listen for requests from the coordinator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLV1FbpLtqtB"
   },
   "source": [
    "### Parameter server training with `Model.fit` API\n",
    "\n",
    "Parameter server training with `Model.fit` API requires the coordinator to use a `tf.distribute.experimental.ParameterServerStrategy` object, and a `tf.keras.utils.experimental.DatasetCreator` as the input. Similar to `Model.fit` usage with no strategy, or with other strategies, the workflow\n",
    "involves creating and compiling the model, preparing the callbacks, followed by a `Model.fit` call.\n",
    "\n",
    "### Parameter server training with custom training loop (CTL) API\n",
    "\n",
    "With CTLs, the `tf.distribute.experimental.coordinator.ClusterCoordinator`\n",
    "class is the key component used for the coordinator. The `ClusterCoordinator`\n",
    "class needs to work in conjunction with a `tf.distribute.Strategy` object. This\n",
    "`tf.distribute.Strategy` object is needed to provide the information of the cluster and is used to define a training step as we have seen in [custom training with `MirroredStrategy`](https://www.tensorflow.org/tutorials/distribute/custom_training#training_loop). The `ClusterCoordinator` object then dispatches the execution of these training\n",
    "steps to remote workers. For parameter server training, the `ClusterCoordinator`\n",
    "needs to work with a `tf.distribute.experimental.ParameterServerStrategy`.\n",
    "\n",
    "The most important API provided by the `ClusterCoordinator` object is `schedule`. The `schedule` API enqueues a `tf.function` and returns a future-like `RemoteValue` immediately. The queued functions will be dispatched to remote workers in background threads and their `RemoteValue`s will be filled asynchronously. Since `schedule` doesn’t require worker assignment, the `tf.function` passed in can be executed on any available worker. If the worker it is executed on becomes unavailable before its completion, the function will be retried on another available worker. Because of this fact and the fact that function execution is not atomic, a function may be executed more than once.\n",
    "\n",
    "In addition to dispatching remote functions, the `ClusterCoordinator` also helps\n",
    "to create datasets on all the workers and rebuild these datasets when a worker recovers from failure.\n",
    "\n",
    "Each learning objective will correspond to a _#TODO_ in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/production_ml/solutions/parameter_server_training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyDnWjmOje5-"
   },
   "source": [
    "## Setup\n",
    "\n",
    "The tutorial will branch into CTL or `Model.fit` paths, and you can choose the\n",
    "one that fits your need. Sections other than \"Training with X\" are appliable to\n",
    "both paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:22:32.810220Z",
     "iopub.status.busy": "2021-05-11T01:22:32.809396Z",
     "iopub.status.idle": "2021-05-11T01:23:03.304189Z",
     "shell.execute_reply": "2021-05-11T01:23:03.303586Z"
    },
    "id": "0-V3LUcIs4a-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "!pip install -q portpicker\n",
    "!pip install --upgrade tensorflow==2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7KBpffWzlxH"
  },
  "source": [
   "**NOTE: Please ignore any incompatibility warnings and errors and re-run the above cell before proceeding.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:03.309077Z",
     "iopub.status.busy": "2021-05-11T01:23:03.308452Z",
     "iopub.status.idle": "2021-05-11T01:23:10.014853Z",
     "shell.execute_reply": "2021-05-11T01:23:10.014277Z"
    },
    "id": "GlI_NAVFae3J"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import portpicker\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers.experimental.preprocessing as kpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses TF2.x.\n",
    "Please check your tensorflow version using the cell below."
   ]
  },
  {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [
  {
   "name": "stdout",
   "output_type": "stream",
   "text": [
    "TensorFlow version:  2.6.0\n"
   ]
  }
 ],
 "source": [
  "# Show the currently installed version of TensorFlow\n",
  "print(\"TensorFlow version: \",tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvwgM2rzgzIC"
   },
   "source": [
    "## Cluster Setup\n",
    "\n",
    "As mentioned above, a parameter server training cluster requires a coordinator task that runs your training program, one or several workers and parameter server tasks that run TensorFlow servers, i.e. `tf.distribute.Server`, and possibly an additional evaluation task that runs side-car evaluation (see the side-car evaluation section below). The\n",
    "requirements to set them up are:\n",
    "\n",
    "*   The coordinator task needs to know the addresses and ports of all other TensorFlow servers except the evaluator.\n",
    "*   The workers and parameter servers need to know which port they need to listen to. For the sake of simplicity, we usually pass in the complete cluster information when we create TensorFlow servers on these tasks.\n",
    "*   The evaluator task doesn’t have to know the setup of the training cluster. If it does, it should not attempt to connect to the training cluster.\n",
    "*   Workers and parameter servers should have task types as “worker” and “ps” respectively. The coordinator should use “chief” as the task type for legacy reasons.\n",
    "\n",
    "In this tutorial, we will create an in-process cluster so that the whole parameter server training can be run in colab. We will introduce how to set up real clusters in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UNs7Lm2g19n"
   },
   "source": [
    "### In-process cluster\n",
    "\n",
    "In this tutorial, we will start a bunch of TensorFlow servers in advance and\n",
    "connect to them later. Note that this is only for the purpose of this tutorial's\n",
    "demonstration, and in real training the servers will be started on worker and ps\n",
    "machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:10.024466Z",
     "iopub.status.busy": "2021-05-11T01:23:10.023784Z",
     "iopub.status.idle": "2021-05-11T01:23:15.483490Z",
     "shell.execute_reply": "2021-05-11T01:23:15.482226Z"
    },
    "id": "FbrP5pXuaoVH"
   },
   "outputs": [],
   "source": [
    "def create_in_process_cluster(num_workers, num_ps):\n",
    "  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\n",
    "  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\n",
    "  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\n",
    "\n",
    "  cluster_dict = {}\n",
    "  cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\n",
    "  if num_ps > 0:\n",
    "    cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\n",
    "\n",
    "  cluster_spec = tf.train.ClusterSpec(cluster_dict)\n",
    "\n",
    "  # Workers need some inter_ops threads to work properly.\n",
    "  worker_config = tf.compat.v1.ConfigProto()\n",
    "  if multiprocessing.cpu_count() < num_workers + 1:\n",
    "    worker_config.inter_op_parallelism_threads = num_workers + 1\n",
    "\n",
    "  for i in range(num_workers):\n",
    "    tf.distribute.Server(\n",
    "        cluster_spec, job_name=\"worker\", task_index=i, config=worker_config,\n",
    "        protocol=\"grpc\")\n",
    "\n",
    "  for i in range(num_ps):\n",
    "    tf.distribute.Server(\n",
    "        cluster_spec, job_name=\"ps\", task_index=i, protocol=\"grpc\")\n",
    "\n",
    "  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n",
    "      cluster_spec, rpc_layer=\"grpc\")\n",
    "  return cluster_resolver\n",
    "\n",
    "# Set the environment variable to allow reporting worker and ps failure to the\n",
    "# coordinator. This is a workaround and won't be necessary in the future.\n",
    "os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\n",
    "\n",
    "NUM_WORKERS = 3\n",
    "NUM_PS = 2\n",
    "cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX_91OByt0J2"
   },
   "source": [
    "The in-process cluster setup is frequently used in our unit testing. Here is\n",
    "[one example](https://github.com/tensorflow/tensorflow/blob/7621d31921c2ed979f212da066631ddfda37adf5/tensorflow/python/distribute/coordinator/cluster_coordinator_test.py#L437)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyby6M2Jqg6J"
   },
   "source": [
    "## Instantiate a `ParameterServerStrategy`\n",
    "\n",
    "Before we dive into the training code, let's instantiate a `ParameterServerStrategy` object. Note that this is needed regardless of whether you are proceeding with a custom training loop or `Model.fit`. `variable_partitioner` argument will be explained in the [next section](#variable-sharding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:15.494786Z",
     "iopub.status.busy": "2021-05-11T01:23:15.493906Z",
     "iopub.status.idle": "2021-05-11T01:23:15.534380Z",
     "shell.execute_reply": "2021-05-11T01:23:15.533385Z"
    },
    "id": "_YyEPgisrC35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:`tf.distribute.experimental.ParameterServerStrategy` is initialized with cluster_spec: ClusterSpec({'ps': ['localhost:18923', 'localhost:24530'], 'worker': ['localhost:19174', 'localhost:22565', 'localhost:23430']})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:ParameterServerStrategyV2 is now connecting to cluster with cluster_spec: ClusterSpec({'ps': ['localhost:18923', 'localhost:24530'], 'worker': ['localhost:19174', 'localhost:22565', 'localhost:23430']})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:chief/replica:0/task:0/device:CPU:0'], variable_device = '/job:chief/replica:0/task:0/device:CPU:0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Number of GPUs on workers: 0\n"
     ]
    }
   ],
   "source": [
    "variable_partitioner = (\n",
    "    tf.distribute.experimental.partitioners.FixedShardsPartitioner(\n",
    "        num_shards=NUM_PS))\n",
    "\n",
    "strategy = tf.distribute.experimental.ParameterServerStrategy(\n",
    "    cluster_resolver,\n",
    "    variable_partitioner=variable_partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlAQxuMDJ3k9"
   },
   "source": [
    "In order to use GPUs for training, allocate GPUs visible to each worker.\n",
    "`ParameterServerStrategy` will use all the available GPUs on each worker,\n",
    "with the restriction that all workers should have the same number of GPUs\n",
    "available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMmBLsf6sEXh"
   },
   "source": [
    "### Variable sharding\n",
    "\n",
    "Variable sharding refers to splitting a variable into multiple smaller\n",
    "variables. We call these smaller variables *shard*s. Variable sharding may be\n",
    "useful to distribute the network load when accessing these shards. It is also\n",
    "useful to distribute computation and storage of a normal variable across\n",
    "multiple parameter servers.\n",
    "\n",
    "To enable variable sharding, you can pass in a `variable_partitioner` when\n",
    "constructing a `ParameterServerStrategy` object. The `variable_partitioner` will\n",
    "be invoked every time when a variable is created and it is expected to return\n",
    "the number of shards along each dimension of the variable. Some out-of-box\n",
    "`variable_partitioner`s are provided such as\n",
    "`tf.distribute.experimental.partitioners.FixedShardsPartitioner`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1--SxlxtsOb7"
   },
   "source": [
    "When a `variable_partitioner` is passed in and if you create a variable directly\n",
    "under `strategy.scope()`, it will become a container type with a `variables`\n",
    "property which provides access to the list of shards. In most cases, this\n",
    "container will be automatically converted to a Tensor by concatenating all the\n",
    "shards. As a result, it can be used as a normal variable. On the other hand,\n",
    "some TensorFlow methods such as `tf.nn.embedding_lookup` provide efficient\n",
    "implementation for this container type and in these methods automatic\n",
    "concatenation will be avoided.\n",
    "\n",
    "Please see the API docstring of `ParameterServerStrategy` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlOq-O-26O1d"
   },
   "source": [
    "## Training with `Model.fit`\n",
    "<a id=\"training_with_modelfit\"></a>\n",
    "\n",
    "Keras provides an easy-to-use training API via `Model.fit` that handles the\n",
    "training loop under the hood, with the flexbility of overridable `train_step`,\n",
    "and callbacks which provide functionalities such as checkpoint saving, or\n",
    "summary saving for TensorBoard. With `Model.fit`, the same training code can be\n",
    "used for other strategies with a simple swap of the strategy object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMZ9Cu5J6ZGi"
   },
   "source": [
    "### Input data\n",
    "\n",
    "`Model.fit` with parameter server training requires that the input data be\n",
    "provided in a callable that takes a single argument of type\n",
    "`tf.distribute.InputContext`, and returns a `tf.data.Dataset`. Then, create a\n",
    "`tf.keras.utils.experimental.DatasetCreator` object that takes such `callable`,\n",
    "and an optional `tf.distribute.InputOptions` object via `input_options`\n",
    "argument. Note that it is recommended to shuffle and repeat the data with\n",
    "parameter server training, and specify `steps_per_epoch` in `fit` call so the library knows the\n",
    "epoch boundaries.\n",
    "\n",
    "Please see\n",
    "[Distributed Input](https://www.tensorflow.org/tutorials/distribute/input#usage_2)\n",
    "guide for more information about the `InputContext` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:15.547599Z",
     "iopub.status.busy": "2021-05-11T01:23:15.546650Z",
     "iopub.status.idle": "2021-05-11T01:23:15.549439Z",
     "shell.execute_reply": "2021-05-11T01:23:15.548940Z"
    },
    "id": "shAo1CCS7wU1"
   },
   "outputs": [],
   "source": [
    "def dataset_fn(input_context):\n",
    "  global_batch_size = 64\n",
    "  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n",
    "  x = tf.random.uniform((10, 10))\n",
    "  y = tf.random.uniform((10,))\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()\n",
    "  dataset = dataset.shard(\n",
    "      input_context.num_input_pipelines, input_context.input_pipeline_id)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(2)\n",
    "  return dataset\n",
    "\n",
    "dc = tf.keras.utils.experimental.DatasetCreator(dataset_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_jhF70K7zON"
   },
   "source": [
    "The code in `dataset_fn` will be invoked on the input device, which is usually\n",
    "the CPU, on each of the worker machines.\n",
    "\n",
    "### Model construction and compiling\n",
    "\n",
    "Now, you will create a `tf.keras.Model` with the APIs of choice (a trivial\n",
    "`tf.keras.models.Sequential` model is being used as a demonstration here),\n",
    "followed by a `Model.compile` call to incorporate components such as optimizer,\n",
    "metrics, or parameters such as `steps_per_execution`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:15.554309Z",
     "iopub.status.busy": "2021-05-11T01:23:15.553684Z",
     "iopub.status.idle": "2021-05-11T01:23:15.600920Z",
     "shell.execute_reply": "2021-05-11T01:23:15.601332Z"
    },
    "id": "PhTHUYaD74vT"
   },
   "outputs": [],
   "source": [
     "# TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWb_Ekm377YX"
   },
   "source": [
    "### Callbacks and training\n",
    "\n",
    "<a id=\"callbacks-and-training\"> </a>\n",
    "\n",
    "Before you call `model.fit` for the actual training, let's prepare the needed\n",
    "callbacks for common tasks such as:\n",
    "\n",
    "*   `ModelCheckpoint` - to save the model weights.\n",
    "\n",
    "*   `BackupAndRestore` - to make sure the training progress is automatically\n",
    "    backed up, and recovered if the cluster experiences unavailability (such as\n",
    "    abort or preemption), or\n",
    "\n",
    "*   `TensorBoard` - to save the progress reports into summary files which get\n",
    "    visualized in TensorBoard tool.\n",
    "\n",
    "Note that due to performance consideration, custom callbacks cannot have batch\n",
    "level callbacks overridden when used with `ParameterServerStrategy`. Please\n",
    "modify your custom callbacks to make them epoch level calls, and adjust\n",
    "`steps_per_epoch` to a suitable value. In addition, `steps_per_epoch` is a\n",
    "required argument for `Model.fit` when used with `ParameterServerStrategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:15.607399Z",
     "iopub.status.busy": "2021-05-11T01:23:15.606628Z",
     "iopub.status.idle": "2021-05-11T01:23:21.971854Z",
     "shell.execute_reply": "2021-05-11T01:23:21.972250Z"
    },
    "id": "3ddUvUZk7_wm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 3s - loss: 0.4391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.4103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "20/20 - 0s - loss: 0.3056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function MultiDeviceSaver.save.<locals>.tf_function_save at 0x7fefd40207b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function MultiDeviceSaver.save.<locals>.tf_function_save at 0x7fee7553b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "20/20 - 0s - loss: 0.2992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "20/20 - 0s - loss: 0.2698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/my_working_dir/ckpt/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fefe405c048>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = '/tmp/my_working_dir'\n",
    "log_dir = os.path.join(working_dir, 'log')\n",
    "ckpt_filepath = os.path.join(working_dir, 'ckpt')\n",
    "backup_dir = os.path.join(working_dir, 'backup')\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),\n",
    "    #tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir),\n",
    "]\n",
    "model.fit(dc, epochs=5, steps_per_epoch=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWgP1h2z8B3j"
   },
   "source": [
    "### Direct usage with `ClusterCoordinator` (optional)\n",
    "\n",
    "Even if you choose `Model.fit` training path, you can optionally instantiate a\n",
    "`ClusterCoordinator` object to schedule other functions you would like to be\n",
    "executed on the workers. See below\n",
    "Training with Custom Training Loop\n",
    "section for more details and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxypEyIthR0z"
   },
   "source": [
    "## Training with Custom Training Loop\n",
    "\n",
    "<a id=\"training_with_custom_training_loop\"> </a>\n",
    "\n",
    "Custom training loop with `tf.distribute.Strategy` \n",
    "provides great flexibility to define training loops. With the `ParameterServerStrategy` defined above, you will use a\n",
    "`ClusterCoordinator` to dispatch the execution of training steps to remote\n",
    "workers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAwLWSliiDRb"
   },
   "source": [
    "Then, you will create a model, define a dataset and a step function as we have\n",
    "seen in the training loop with other `tf.distribute.Strategy`s. You can find\n",
    "more details in this\n",
    "[tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training).\n",
    "\n",
    "To ensure efficient dataset prefetching, use the recommended \n",
    "distributed dataset creation APIs mentioned in the\n",
    "[Dispatch Training steps to remote workers](https://www.tensorflow.org/tutorials/distribute/parameter_server_training#dispatch_training_steps_to_remote_workers)\n",
    "section below. Also, make sure to call `strategy.run` inside worker_fn \n",
    "to take full advantage of GPUs allocated on workers. Rest of the steps \n",
    "are the same for training with or without GPUs.\n",
    "\n",
    "Let’s create these components in following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QNkCtV8VivM"
   },
   "source": [
    "### Setup the data\n",
    "First, write a function that creates a dataset that includes preprocessing logic implemented by Keras preprocessing layers. We will create these layers outside the `dataset_fn` but apply the transformation inside the `dataset_fn` since you will wrap the `dataset_fn` into a `tf.function` which doesn't allow variables to be created inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:21.985208Z",
     "iopub.status.busy": "2021-05-11T01:23:21.984599Z",
     "iopub.status.idle": "2021-05-11T01:23:22.010510Z",
     "shell.execute_reply": "2021-05-11T01:23:22.010002Z"
    },
    "id": "2GUwATssauus"
   },
   "outputs": [],
   "source": [
    "feature_vocab = [\n",
    "    \"avenger\", \"ironman\", \"batman\", \"hulk\", \"spiderman\", \"kingkong\",\n",
    "    \"wonder_woman\"\n",
    "]\n",
    "label_vocab = [\"yes\", \"no\"]\n",
    "\n",
    "with strategy.scope():\n",
    "  feature_lookup_layer = kpl.StringLookup(vocabulary=feature_vocab)\n",
    "\n",
    "  label_lookup_layer = kpl.StringLookup(vocabulary=label_vocab,\n",
    "                                        num_oov_indices=0,\n",
    "                                        mask_token=None)\n",
    "\n",
    "  raw_feature_input = keras.layers.Input(\n",
    "      shape=(3,), dtype=tf.string, name=\"feature\")\n",
    "  feature_id_input = feature_lookup_layer(raw_feature_input)\n",
    "  feature_preprocess_stage = keras.Model(\n",
    "      {\"features\": raw_feature_input}, feature_id_input)\n",
    "\n",
    "  raw_label_input = keras.layers.Input(\n",
    "      shape=(1,), dtype=tf.string, name=\"label\")\n",
    "  label_id_input = label_lookup_layer(raw_label_input)\n",
    "  label_preprocess_stage = keras.Model({\"label\": raw_label_input}, label_id_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgp8MX_7OR_A"
   },
   "source": [
    "Generate toy examples in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.016870Z",
     "iopub.status.busy": "2021-05-11T01:23:22.016259Z",
     "iopub.status.idle": "2021-05-11T01:23:22.018569Z",
     "shell.execute_reply": "2021-05-11T01:23:22.018068Z"
    },
    "id": "chIY4fFANaFH"
   },
   "outputs": [],
   "source": [
    "def feature_and_label_gen(num_examples=200):\n",
    "  examples = {\"features\": [], \"label\": []}\n",
    "  for _ in range(num_examples):\n",
    "    features = random.sample(feature_vocab, 3)\n",
    "    label = [\"yes\"] if \"avenger\" in features else [\"no\"]\n",
    "    examples[\"features\"].append(features)\n",
    "    examples[\"label\"].append(label)\n",
    "  return examples\n",
    "\n",
    "examples = feature_and_label_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AtZBya7OeyZ"
   },
   "source": [
    "Then we create the training dataset wrapped in a dataset_fn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.023143Z",
     "iopub.status.busy": "2021-05-11T01:23:22.022571Z",
     "iopub.status.idle": "2021-05-11T01:23:22.024302Z",
     "shell.execute_reply": "2021-05-11T01:23:22.024658Z"
    },
    "id": "Gs0QYRZoNbvw"
   },
   "outputs": [],
   "source": [
    "def dataset_fn(_):\n",
    "  raw_dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "\n",
    "  train_dataset = # TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT9PQexJiFtB"
   },
   "source": [
    "### Build the model\n",
    "Second, we create the model and other objects. Make sure to create all variables\n",
    "under `strategy.scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.031739Z",
     "iopub.status.busy": "2021-05-11T01:23:22.031124Z",
     "iopub.status.idle": "2021-05-11T01:23:22.094343Z",
     "shell.execute_reply": "2021-05-11T01:23:22.094706Z"
    },
    "id": "Quxud1uEazeo"
   },
   "outputs": [],
   "source": [
    "# These variables created under the `strategy.scope` will be placed on parameter\n",
    "# servers in a round-robin fashion.\n",
    "with strategy.scope():\n",
    "  # Create the model. The input needs to be compatible with KPLs.\n",
    "  # TODO: Your code goes here\n",
    "\n",
    "  emb_layer = keras.layers.Embedding(\n",
    "      input_dim=len(feature_lookup_layer.get_vocabulary()), output_dim=20)\n",
    "  emb_output = tf.reduce_mean(emb_layer(model_input), axis=1)\n",
    "  dense_output = keras.layers.Dense(units=1, activation=\"sigmoid\")(emb_output)\n",
    "  model = keras.Model({\"features\": model_input}, dense_output)\n",
    "\n",
    "  optimizer = keras.optimizers.RMSprop(learning_rate=0.1)\n",
    "  accuracy = keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyuxiqCQU50m"
   },
   "source": [
    "Let's confirm that the use of `FixedShardsPartitioner` split all variables into two shards and each shard was assigned to different parameter servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.099682Z",
     "iopub.status.busy": "2021-05-11T01:23:22.098982Z",
     "iopub.status.idle": "2021-05-11T01:23:22.101348Z",
     "shell.execute_reply": "2021-05-11T01:23:22.100896Z"
    },
    "id": "04r1nO4WVDO1"
   },
   "outputs": [],
   "source": [
    "assert len(emb_layer.weights) == 2\n",
    "#assert emb_layer.weights[0].shape == (4, 20)\n",
    "assert emb_layer.weights[1].shape == (4, 20)\n",
    "assert emb_layer.weights[0].device == \"/job:ps/replica:0/task:0/device:CPU:0\"\n",
    "assert emb_layer.weights[1].device == \"/job:ps/replica:0/task:1/device:CPU:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWhfXZLRiHyM"
   },
   "source": [
    "### Define the training step\n",
    "Third, create the training step wrapped into a `tf.function`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.107724Z",
     "iopub.status.busy": "2021-05-11T01:23:22.107140Z",
     "iopub.status.idle": "2021-05-11T01:23:22.109132Z",
     "shell.execute_reply": "2021-05-11T01:23:22.109476Z"
    },
    "id": "aNNVo0bFa1K9"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def step_fn(iterator):\n",
    "\n",
    "  def replica_fn(batch_data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(batch_data, training=True)\n",
    "      per_example_loss = keras.losses.BinaryCrossentropy(\n",
    "              reduction=tf.keras.losses.Reduction.NONE)(labels, pred)\n",
    "      loss = tf.nn.compute_average_loss(per_example_loss)\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
    "    accuracy.update_state(labels, actual_pred)\n",
    "    return loss\n",
    "\n",
    "  batch_data, labels = next(iterator)\n",
    "  losses = strategy.run(replica_fn, args=(batch_data, labels))\n",
    "  return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvrYQUeYiLNy"
   },
   "source": [
    "In the above step function, calling `strategy.run` and `strategy.reduce` in the\n",
    "`step_fn` can support multiple GPUs per worker. If the workers have GPUs\n",
    "allocated, `strategy.run` will distribute the datasets on multiple replicas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPJ3PV_L2zAY"
   },
   "source": [
    "### Dispatch training steps to remote workers\n",
    "<a id=\"dispatch_training_steps_to_remote_workers\"> </a>\n",
    "\n",
    "After all the computations are defined by `ParameterServerStrategy`, we will use\n",
    "the `ClusterCoordinator` class to create resources and distribute the training\n",
    "steps to remote workers.\n",
    "\n",
    "Let’s first create a `ClusterCoordinator` object and pass in the strategy\n",
    "object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.114002Z",
     "iopub.status.busy": "2021-05-11T01:23:22.113412Z",
     "iopub.status.idle": "2021-05-11T01:23:22.115828Z",
     "shell.execute_reply": "2021-05-11T01:23:22.115392Z"
    },
    "id": "DpcMlH7Pa3DB"
   },
   "outputs": [],
   "source": [
    "coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xRIgKxciOSe"
   },
   "source": [
    "Then we create a per-worker dataset and an iterator. In the `per_worker_dataset_fn` below, wrapping the `dataset_fn` into\n",
    "`strategy.distribute_datasets_from_function` is recommended to allow efficient\n",
    "prefetching to GPUs seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.133778Z",
     "iopub.status.busy": "2021-05-11T01:23:22.128461Z",
     "iopub.status.idle": "2021-05-11T01:23:22.193650Z",
     "shell.execute_reply": "2021-05-11T01:23:22.193108Z"
    },
    "id": "h9DCvTJTa4Q2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.string, name='feature'), name='feature', description=\"created by layer 'feature'\"), but it was called on an input with incompatible shape (3,).\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def per_worker_dataset_fn():\n",
    "  return strategy.distribute_datasets_from_function(dataset_fn)\n",
    "\n",
    " # TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2pnOx78iRwW"
   },
   "source": [
    "The final step is to distribute the computation to remote workers using `schedule`. The `schedule` method enqueues a `tf.function` and returns a future-like `RemoteValue` immediately. The queued functions will be dispatched to remote workers in background threads and the `RemoteValue` will be filled asynchronously. The `join` method can be used to wait until all scheduled functions are excuted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:22.198884Z",
     "iopub.status.busy": "2021-05-11T01:23:22.198159Z",
     "iopub.status.idle": "2021-05-11T01:23:25.781104Z",
     "shell.execute_reply": "2021-05-11T01:23:25.780363Z"
    },
    "id": "gmPvactfa6Eh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, accuracy is 0.818750.\n",
      "Finished epoch 1, accuracy is 1.000000.\n",
      "Finished epoch 2, accuracy is 1.000000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3, accuracy is 1.000000.\n"
     ]
    }
   ],
   "source": [
    "num_epoches = 4\n",
    "steps_per_epoch = 5\n",
    "for i in range(num_epoches):\n",
    "  accuracy.reset_states()\n",
    "  for _ in range(steps_per_epoch):\n",
    "    coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
    "  # Wait at epoch boundaries.\n",
    "  coordinator.join()\n",
    "  print (\"Finished epoch %d, accuracy is %f.\" % (i, accuracy.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBn-gn-OP3DR"
   },
   "source": [
    "Here is how you can fetch the result of a `RemoteValue`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:25.790282Z",
     "iopub.status.busy": "2021-05-11T01:23:25.788988Z",
     "iopub.status.idle": "2021-05-11T01:23:25.798205Z",
     "shell.execute_reply": "2021-05-11T01:23:25.797579Z"
    },
    "id": "-15a2I_lQDO1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss is 0.004459\n"
     ]
    }
   ],
   "source": [
   "# TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htY4QKc9iXg9"
   },
   "source": [
    "Alternatively, you can launch all steps and do something while waiting for\n",
    "completion:\n",
    "\n",
    "```Python\n",
    "for _ in range(total_steps):\n",
    "  coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
    "while not coordinator.done():\n",
    "  time.sleep(10)\n",
    "  # Do something like logging metrics or writing checkpoints.\n",
    "```\n",
    "\n",
    "For the complete training and serving workflow for this particular example,\n",
    "please check out this\n",
    "[test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/distribute/parameter_server_evaluation_test.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzNsj2GR3BGs"
   },
   "source": [
    "### More about dataset creation\n",
    "\n",
    "The dataset in the above code is created using the `create_per_worker_dataset`\n",
    "API. It creates one dataset per worker and returns a container object. You can\n",
    "call `iter` method on it to create a per-worker iterator. The per-worker\n",
    "iterator contains one iterator per worker and the corresponding slice of a\n",
    "worker will be substituted in the input argument of the function passed to the\n",
    "`schedule` method before the function is executed on a particular worker.\n",
    "\n",
    "Currently the `schedule` method assumes workers are equivalent and thus assumes\n",
    "the datasets on different workers are the same except they may be shuffled\n",
    "differently if they contain a\n",
    "[dataset.shuffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)\n",
    "operation. Because of this, we also recommend the datasets to be repeated\n",
    "indefinitely and schedule a finite number of steps instead of relying on the\n",
    "`OutOfRangeError` from a dataset.\n",
    "\n",
    "Another important note is that `tf.data` datasets don’t support implicit\n",
    "serialization and deserialization across task boundaries. So it is important to\n",
    "create the whole dataset inside the function passed to\n",
    "`create_per_worker_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcfdI_M83lAM"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "There are more than one way to define and run an evaluation loop in distributed training. Each has its own pros and cons as described below. The inline evaluation method is recommended if you don't have a preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiG8EhcY3gA1"
   },
   "source": [
    "### Inline evaluation\n",
    "\n",
    "In this method the coordinator alternates between training and evaluation and thus we call it inline evaluation. There are several benefits of inline evaluation. For example, it can support large evaluation models and evaluation datasets that a single task cannot hold. For another example, the evaluation results can be used to make decisions for training next epoch.\n",
    "\n",
    "There are two ways to implement inline evaluation:\n",
    "\n",
    "- **Direct evaluation** - For small models and evaluation datasets the coordinator can run evaluation directly on the distributed model with the evaluation dataset on the coordinator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:25.808342Z",
     "iopub.status.busy": "2021-05-11T01:23:25.806851Z",
     "iopub.status.idle": "2021-05-11T01:23:25.884125Z",
     "shell.execute_reply": "2021-05-11T01:23:25.883327Z"
    },
    "id": "WakiAakoaHVn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.string, name='feature'), name='feature', description=\"created by layer 'feature'\"), but it was called on an input with incompatible shape (3,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      feature_and_label_gen(num_examples=16)).map(\n",
    "          lambda x: (\n",
    "              {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
    "              label_preprocess_stage(x[\"label\"])\n",
    "          )).batch(8)\n",
    "\n",
    "eval_accuracy = keras.metrics.Accuracy()\n",
    "for batch_data, labels in eval_dataset:\n",
    "  pred = model(batch_data, training=False)\n",
    "  actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
    "  eval_accuracy.update_state(labels, actual_pred)\n",
    "\n",
    "print (\"Evaluation accuracy: %f\" % eval_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKGHbdI7aGoJ"
   },
   "source": [
    "- **Distributed evaluation** - For large models or datasets that are infeasible to run directly on the coordinator, the coordinator task can distribute evaluation tasks to the workers via the `schedule`/`join` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T01:23:25.895536Z",
     "iopub.status.busy": "2021-05-11T01:23:25.892663Z",
     "iopub.status.idle": "2021-05-11T01:23:27.766943Z",
     "shell.execute_reply": "2021-05-11T01:23:27.766474Z"
    },
    "id": "XcHNHJpDgEvK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.string, name='feature'), name='feature', description=\"created by layer 'feature'\"), but it was called on an input with incompatible shape (3,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  # Define the eval metric on parameter servers.\n",
    "  eval_accuracy = keras.metrics.Accuracy()\n",
    "\n",
    "@tf.function\n",
    "def eval_step(iterator):\n",
    "  def replica_fn(batch_data, labels):\n",
    "    pred = model(batch_data, training=False)\n",
    "    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
    "    eval_accuracy.update_state(labels, actual_pred)\n",
    "  batch_data, labels = next(iterator)\n",
    "  strategy.run(replica_fn, args=(batch_data, labels))\n",
    "\n",
    "def eval_dataset_fn():\n",
    "  return tf.data.Dataset.from_tensor_slices(\n",
    "      feature_and_label_gen(num_examples=16)).map(\n",
    "          lambda x: (\n",
    "              {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
    "              label_preprocess_stage(x[\"label\"])\n",
    "          )).shuffle(16).repeat().batch(8)\n",
    "\n",
    "per_worker_eval_dataset = coordinator.create_per_worker_dataset(eval_dataset_fn)\n",
    "per_worker_eval_iterator = iter(per_worker_eval_dataset)\n",
    "\n",
    "eval_steps_per_epoch = 2\n",
    "for _ in range(eval_steps_per_epoch):\n",
    "  coordinator.schedule(eval_step, args=(per_worker_eval_iterator,))\n",
    "coordinator.join()\n",
    "print (\"Evaluation accuracy: %f\" % eval_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKrQktZX5z7a"
   },
   "source": [
    "Note: currently the `schedule`/`join` methods don’t support visitation guarantee or exactly-once semantics. In other words, there is no guarantee that all evaluation examples in a dataset will be evaluated exactly once; some may not be visited and some may be evaluated multiple times. Visitation guarantee on evaluation dataset is being worked on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H40X-9Gs3i7_"
   },
   "source": [
    "### Side-car evaluation\n",
    "\n",
    "Another method is called side-car evaluation which is to create a dedicated evaluator task that repeatedly reads checkpoints and runs evaluation on a latest checkpoint. It allows your training program to finish early if you don't need to change your training loop based on evaluation results. However, it requires an additional evaluator task and periodic checkpointing to trigger evaluation. Following is a possible side-car evaluation loop:\n",
    "\n",
    "```Python\n",
    "checkpoint_dir = ...\n",
    "eval_model = ...\n",
    "eval_data = ...\n",
    "checkpoint = tf.train.Checkpoint(model=eval_model)\n",
    "\n",
    "for latest_checkpoint in tf.train.checkpoints_iterator(\n",
    "    checkpoint_dir):\n",
    "  try:\n",
    "    checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "  except (tf.errors.OpError,) as e:\n",
    "    # checkpoint may be deleted by training when it is about to read it.\n",
    "    continue\n",
    "\n",
    "  # Optionally add callbacks to write summaries.\n",
    "  eval_model.evaluate(eval_data)\n",
    "\n",
    "  # Evaluation finishes when it has evaluated the last epoch.\n",
    "  if latest_checkpoint.endswith('-{}'.format(train_epoches)):\n",
    "    break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TkNbtpPhFRQ"
   },
   "source": [
    "## Clusters in Real-world\n",
    "<a id=\"real_clusters\"></a>\n",
    "\n",
    "Note: this section is not necessary for running the tutorial code in this page.\n",
    "\n",
    "In a real production environment, you will run all tasks in different processes\n",
    "on different machines. The simplest way to configure cluster information on each\n",
    "task is to set \"TF_CONFIG\" environment variables and use a\n",
    "`tf.distribute.cluster_resolver.TFConfigClusterResolver` to parse \"TF_CONFIG\".\n",
    "For a general description about \"TF_CONFIG\" environment variables, please see\n",
    "the [distributed training guide](https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable).\n",
    "\n",
    "If you start your training tasks using Kubernetes or other configuration templates, it is very likely that these templates have already set “TF_CONFIG” for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7AK9SJGt3tQ"
   },
   "source": [
    "### Set “TF_CONFIG” environment variable\n",
    "\n",
    "Suppose you have 3 workers and 2 parameter servers, the “TF_CONFIG” of worker 1\n",
    "can be:\n",
    "\n",
    "```Python\n",
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    \"cluster\": {\n",
    "        \"worker\": [\"host1:port\", \"host2:port\", \"host3:port\"],\n",
    "        \"ps\": [\"host4:port\", \"host5:port\"],\n",
    "        \"chief\": [\"host6:port\"]\n",
    "    },\n",
    "    \"task\": {\"type\": \"worker\", \"index\": 1}\n",
    "})\n",
    "```\n",
    "\n",
    "The “TF_CONFIG” of the evaluator can be:\n",
    "\n",
    "```Python\n",
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    \"cluster\": {\n",
    "        \"evaluator\": [\"host7:port\"]\n",
    "    },\n",
    "    \"task\": {\"type\": \"evaluator\", \"index\": 0}\n",
    "})\n",
    "```\n",
    "\n",
    "The “cluster” part in the above “TF_CONFIG” string for the evaluator is\n",
    "optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZRjMS0pt1LM"
   },
   "source": [
    "### If you use the same binary for all tasks\n",
    "\n",
    "If you prefer to run all these tasks using a single binary, you will need to let\n",
    "your program branch into different roles at the very beginning:\n",
    "\n",
    "```Python\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "if cluster_resolver.task_type in (\"worker\", \"ps\"):\n",
    "  # start a TensorFlow server and wait.\n",
    "elif cluster_resolver.task_type == \"evaluator\":\n",
    "  # run side-car evaluation\n",
    "else:\n",
    "  # run the coordinator.\n",
    "```\n",
    "\n",
    "The following code starts a TensorFlow server and waits:\n",
    "\n",
    "```Python\n",
    "# Set the environment variable to allow reporting worker and ps failure to the\n",
    "# coordinator. This is a workaround and won't be necessary in the future.\n",
    "os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\n",
    "\n",
    "server = tf.distribute.Server(\n",
    "    cluster_resolver.cluster_spec(),\n",
    "    job_name=cluster_resolver.task_type,\n",
    "    task_index=cluster_resolver.task_id,\n",
    "    protocol=cluster_resolver.rpc_layer or \"grpc\",\n",
    "    start=True)\n",
    "server.join()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWdYfK593eOL"
   },
   "source": [
    "## Handling Task Failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl9eK5r13cOv"
   },
   "source": [
    "### Worker failure\n",
    "\n",
    "`ClusterCoordinator` or `Model.fit` provides built-in fault tolerance for worker\n",
    "failure. Upon worker recovery, the previously provided dataset function (either\n",
    "to `create_per_worker_dataset` for CTL, or `DatasetCreator` for `Model.fit`)\n",
    "will be invoked on the workers to re-create the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP0OHZ1-Ne-B"
   },
   "source": [
    "### Parameter server or coordinator failure\n",
    "\n",
    "However, when the coordinator sees a parameter server error, it will raise an `UnavailableError` or `AbortedError` immediately. You can restart the coordinator in this case. The coordinator itself can also become unavailable. Therefore, certain tooling is recommended in order to not lose the training progress:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7m7Itoz8lsI"
   },
   "source": [
    "*   For `Model.fit`, you should use a `BackupAndRestore` callback, which handles\n",
    "    the progress saving and restoration automatically. See\n",
    "    [Callbacks and training](#callbacks-and-training) section above for an\n",
    "    example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XlLyJp53Z8A"
   },
   "source": [
    "*   For CTLs, you should checkpoint the model variables periodically and load\n",
    "    model variables from a checkpoint, if any, before training starts. The\n",
    "    training progress can be inferred approximately from `optimizer.iterations`\n",
    "    if an optimizer is checkpointed:\n",
    "\n",
    "```Python\n",
    "checkpoint_manager = tf.train.CheckpointManager(\n",
    "    tf.train.Checkpoint(model=model, optimizer=optimizer),\n",
    "    checkpoint_dir,\n",
    "    max_to_keep=3)\n",
    "if checkpoint_manager.latest_checkpoint:\n",
    "  checkpoint = checkpoint_manager.checkpoint\n",
    "  checkpoint.restore(\n",
    "      checkpoint_manager.latest_checkpoint).assert_existing_objects_matched()\n",
    "\n",
    "global_steps = int(optimizer.iterations.numpy())\n",
    "starting_epoch = global_steps // steps_per_epoch\n",
    "\n",
    "for _ in range(starting_epoch, num_epoches):\n",
    "  for _ in range(steps_per_epoch):\n",
    "    coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
    "  coordinator.join()\n",
    "  checkpoint_manager.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlN1P7C53XK9"
   },
   "source": [
    "### Fetching a `RemoteValue`\n",
    "\n",
    "Fetching a `RemoteValue` is guaranteed to succeed if a function is executed\n",
    "successfully. This is because currently the return value is immediately copied\n",
    "to the coordinator after a function is executed. If there is any worker failure\n",
    "during the copy, the function will be retried on another available worker.\n",
    "Therefore, if you want to optimize for performance, you can schedule functions\n",
    "without a return value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZcR_xNZ3UdU"
   },
   "source": [
    "## Error Reporting\n",
    "\n",
    "Once the coordinator sees an error such as `UnavailableError` from parameter\n",
    "servers or other application errors such as an `InvalidArgument` from\n",
    "`tf.debugging.check_numerics`, it will cancel all pending and queued functions\n",
    "before raising the error. Fetching their corresponding `RemoteValue`s will raise\n",
    "a `CancelledError`.\n",
    "\n",
    "After an error is raised, the coordinator will not raise the same error or any\n",
    "error from cancelled functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfhbXH-j3NVw"
   },
   "source": [
    "## Performance Improvement\n",
    "\n",
    "There are several possible reasons if you see performance issues when you train\n",
    "with `ParameterServerStrategy` and `ClusterResolver`.\n",
    "\n",
    "One common reason is parameter servers have unbalanced load and some\n",
    "heavily-loaded parameter servers have reached capacity. There can also be\n",
    "multiple root causes. Some simple methods to mitigate this issue are to\n",
    "\n",
    "1.  shard your large model variables via specifying a `variable_partitioner`\n",
    "    when constructing a `ParameterServerStrategy`.\n",
    "2.  avoid creating a hotspot variable that is required by all parameter servers\n",
    "    in a single step if possible. For example, use a constant learning rate\n",
    "    or subclass `tf.keras.optimizers.schedules.LearningRateSchedule` in\n",
    "    optimizers since the default behavior is that the learning rate will become\n",
    "    a variable placed on a particular parameter server and requested by all\n",
    "    other parameter servers in each step.\n",
    "3.  shuffle your large vocabularies before passing them to Keras preprocessing\n",
    "    layers.\n",
    "\n",
    "Another possible reason for performance issues is the coordinator. Our first\n",
    "implementation of `schedule`/`join` is Python-based and thus may have threading\n",
    "overhead. Also the latency between the coordinator and the workers can be large.\n",
    "If this is the case,\n",
    "\n",
    "*   For `Model.fit`, you can set `steps_per_execution` argument provided at\n",
    "    `Model.compile` to a value larger than 1.\n",
    "\n",
    "*   For CTLs, you can pack multiple steps into a single `tf.function`:\n",
    "\n",
    "```\n",
    "steps_per_invocation = 10\n",
    "@tf.function\n",
    "def step_fn(iterator):\n",
    "  for _ in range(steps_per_invocation):\n",
    "    features, labels = next(iterator)\n",
    "    def replica_fn(features, labels):\n",
    "      ...\n",
    "\n",
    "    strategy.run(replica_fn, args=(features, labels))\n",
    "```\n",
    "\n",
    "As we continue to optimize the library, we hope most users don’t have to\n",
    "manually pack steps in the future.\n",
    "\n",
    "In addition, a small trick for performance improvement is to schedule functions\n",
    "without a return value as explained in the handling task failure section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chu5F7M_JmVk"
   },
   "source": [
    "## Known Limitations\n",
    "\n",
    "Most of the known limitations are covered in above sections. This section\n",
    "provides a summary.\n",
    "\n",
    "### `ParameterServerStrategy` general\n",
    "\n",
    "* `os.environment[\"grpc_fail_fast\"]=\"use_caller\"` is needed on every task, including the coordinator, to make fault tolerance work properly. \n",
    "* Synchronous parameter server training is not supported.\n",
    "* It is usually necessary to pack multiple steps into a single function to achieve optimal performance.\n",
    "* It is not supported to load a saved_model via `tf.saved_model.load` containing sharded variables. Note loading such a saved_model using TensorFlow Serving is expected to work.\n",
    "* It is not supported to load a checkpoint containg sharded optimizer slot variables into a different number of shards.\n",
    "* It is not supported to recover from parameter server failure without restarting the coordinator task.\n",
    "\n",
    "### `Model.fit` specifics\n",
    "\n",
    "*   `steps_per_epoch` argument is required in `Model.fit`. You can select a\n",
    "    value that provides appropriate intervals in an epoch.\n",
    "*   `ParameterServerStrategy` does not have support for custom callbacks that\n",
    "    have batch-level calls for performance reason. You should convert those\n",
    "    calls into epoch-level calls with suitably picked `steps_per_epoch`, so that\n",
    "    they are called every `steps_per_epoch` number of steps. Built-in callbacks\n",
    "    are not affected: their batch-level calls have been modified to be\n",
    "    performant. Supporting batch-level calls for `ParameterServerStrategy` is\n",
    "    being planned.\n",
    "*   For the same reason, unlike other strategies, progress bar and metrics are\n",
    "    logged only at epoch boundaries.\n",
    "*   Input for `Model.fit` only takes the type `DatasetCreator`.\n",
    "*   `run_eagerly` is not supported.\n",
    "*   Evaluation in `Model.fit` is not yet supported. This is one of the\n",
    "    priorities.\n",
    "*   `Model.evaluate` and `Model.predict` are not yet supported.\n",
    "\n",
    "### Custom Training Loop specifics\n",
    "\n",
    "*   `ClusterCoordinator.schedule` doesn't support visitation guarantees for a dataset.\n",
    "*   When `ClusterCoordinator.create_per_worker_dataset` is used, the whole dataset must be created inside the function passed to it.\n",
    "*   `tf.data.Options` is ignored in dataset created by `ClusterCoordinator.create_per_worker_dataset`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "parameter_server_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
