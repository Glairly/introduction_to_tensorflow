{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigquery.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# End-to-End Example for BigQuery TensorFlow Reader\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will use [BigQuery TensorFlow reader](https://github.com/tensorflow/io/tree/master/tensorflow_io/bigquery) and Keras sequential API to train a neural network.\n",
    "\n",
    "## Learning Objective\n",
    "\n",
    "* Import census data into BigQuery.\n",
    "* Load census data in TensorFlow dataset using BigQuery reader.\n",
    "* Build and train a model.\n",
    "* Evaluate the model.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial shows how to use [BigQuery TensorFlow reader](https://github.com/tensorflow/io/tree/master/tensorflow_io/bigquery) and Keras sequential API to train a neural network.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/bigquery.ipynb).\n",
    "\n",
    "**Make sure to enable the BigQueryStorage API.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upgCc3gXybsA"
   },
   "source": [
    "Install required Packages, and restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QeBQuayhuvhg"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # Use the Colab's preinstalled TensorFlow 2.x\n",
    "  %tensorflow_version 2.x \n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tu01THzWcE-J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastavro in /opt/conda/lib/python3.7/site-packages (1.4.4)\n",
      "Collecting tensorflow-io==0.9.0\n",
      "  Downloading tensorflow_io-0.9.0-cp37-cp37m-manylinux2010_x86_64.whl (41.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 41.6 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow<2.1.0,>=2.0.0\n",
      "  Downloading tensorflow-2.0.4-cp37-cp37m-manylinux2010_x86_64.whl (86.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 86.4 MB 63.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.13.0)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 8.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.16.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.38.1)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
      "\u001b[K     |████████████████████████████████| 449 kB 51.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.12.1)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 54.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.1.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.3.0)\n",
      "Collecting h5py<=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 47.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.1.2)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 43.5 MB/s eta 0:00:01    |████████████▏                   | 7.7 MB 43.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.16.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.37.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (57.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->tensorflow-io==0.9.0) (3.5.0)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=c9a717d55a7fdcdf6253c869e440e30a027cba92df7ebc2d6689e5a888e9afd4\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "Successfully built gast\n",
      "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, keras-applications, gast, astor, tensorflow, tensorflow-io\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "  Attempting uninstall: tensorflow-io\n",
      "    Found existing installation: tensorflow-io 0.18.0\n",
      "    Uninstalling tensorflow-io-0.18.0:\n",
      "      Successfully uninstalled tensorflow-io-0.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.19.1 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15.2, but you have tensorflow 2.0.4 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2, but you have tensorflow 2.0.4 which is incompatible.\n",
      "tensorflow-serving-api 2.6.0 requires tensorflow<3,>=2.6.0, but you have tensorflow 2.0.4 which is incompatible.\n",
      "tensorflow-probability 0.13.0rc0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
      "tensorflow-cloud 0.1.14 requires tensorboard>=2.3.0, but you have tensorboard 2.0.2 which is incompatible.\n",
      "apache-beam 2.32.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
      "apache-beam 2.32.0 requires pyarrow<5.0.0,>=0.15.1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "apache-beam 2.32.0 requires typing-extensions<3.8.0,>=3.7.0, but you have typing-extensions 3.10.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed astor-0.8.1 gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 numpy-1.18.5 tensorboard-2.0.2 tensorflow-2.0.4 tensorflow-estimator-2.0.1 tensorflow-io-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install fastavro\n",
    "!pip install tensorflow-io==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ignore"
   },
   "source": [
    "**Please ignore the incompatible errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YUj0878jPyz7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery-storage in /opt/conda/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (21.0)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (0.3.20)\n",
      "Requirement already satisfied: proto-plus>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (1.19.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (1.31.2)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (57.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (1.16.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (1.35.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (2021.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (1.53.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (1.38.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (4.2.2)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (0.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (3.10.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-bigquery-storage) (1.26.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "# Install the specified package\n",
    "!pip install google-cloud-bigquery-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czF1KlC6y8fB"
   },
   "source": [
    "Set your PROJECT ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fbQR-bT_xgba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "env: GCLOUD_PROJECT=qwiklabs-gcp-04-69798a999f35\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"<YOUR PROJECT>\" #@param {type:\"string\"}\n",
    "! gcloud config set project $PROJECT_ID\n",
    "%env GCLOUD_PROJECT=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOqN91M4y_9Y"
   },
   "source": [
    "Import Python libraries, define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G3p2ICG80Z1t"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "\n",
    "LOCATION = 'us'\n",
    "\n",
    "# Storage directory\n",
    "DATA_DIR = os.path.join(tempfile.gettempdir(), 'census_data')\n",
    "\n",
    "# Download options.\n",
    "DATA_URL = 'https://storage.googleapis.com/cloud-samples-data/ml-engine/census/data'\n",
    "TRAINING_FILE = 'adult.data.csv'\n",
    "EVAL_FILE = 'adult.test.csv'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
    "\n",
    "DATASET_ID = 'census_dataset'\n",
    "TRAINING_TABLE_ID = 'census_training_table'\n",
    "EVAL_TABLE_ID = 'census_eval_table'\n",
    "\n",
    "CSV_SCHEMA = [\n",
    "      bigquery.SchemaField(\"age\", \"FLOAT64\"),\n",
    "      bigquery.SchemaField(\"workclass\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"fnlwgt\", \"FLOAT64\"),\n",
    "      bigquery.SchemaField(\"education\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"education_num\", \"FLOAT64\"),\n",
    "      bigquery.SchemaField(\"marital_status\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"occupation\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"relationship\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"race\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"gender\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"capital_gain\", \"FLOAT64\"),\n",
    "      bigquery.SchemaField(\"capital_loss\", \"FLOAT64\"),\n",
    "      bigquery.SchemaField(\"hours_per_week\", \"FLOAT64\"),\n",
    "      bigquery.SchemaField(\"native_country\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"income_bracket\", \"STRING\"),\n",
    "  ]\n",
    "\n",
    "UNUSED_COLUMNS = [\"fnlwgt\", \"education_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooBfVnd-Xxd-"
   },
   "source": [
    "## Import census data into BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qt6wUD_2XFT"
   },
   "source": [
    "Define helper methods to load data into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7mMI7uW_2vP5"
   },
   "outputs": [],
   "source": [
    "def create_bigquery_dataset_if_necessary(dataset_id):\n",
    "  # Construct a full Dataset object to send to the API.\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  dataset = bigquery.Dataset(bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id))\n",
    "  dataset.location = LOCATION\n",
    "\n",
    "  try:\n",
    "    # Constructs the API request\n",
    "    dataset = # TODO -- Your code goes here\n",
    "    return True\n",
    "  except GoogleAPIError as err:\n",
    "    if err.code != 409: # http_client.CONFLICT\n",
    "      raise\n",
    "  return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y3Cr-DEfwNhK"
   },
   "outputs": [],
   "source": [
    "def load_data_into_bigquery(url, table_id):\n",
    "  create_bigquery_dataset_if_necessary(DATASET_ID)\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  dataset_ref = client.dataset(DATASET_ID)\n",
    "  table_ref = dataset_ref.table(table_id)\n",
    "  job_config = bigquery.LoadJobConfig()\n",
    "  job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "  job_config.source_format = bigquery.SourceFormat.CSV\n",
    "  job_config.schema = CSV_SCHEMA\n",
    "\n",
    "  # Constructs the Job to load data into table\n",
    "  load_job = # TODO -- Your code goes here(\n",
    "      url, table_ref, job_config=job_config\n",
    "  )\n",
    "  print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "  load_job.result()  # Waits for table load to complete.\n",
    "  print(\"Job finished.\")\n",
    "\n",
    "  destination_table = client.get_table(table_ref)\n",
    "  print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSA0RIAZZEFZ"
   },
   "source": [
    "Load Census data in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wFZcK03-YDm4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 74b20d98-d84f-4560-b59c-ac8e4b14a0d3\n",
      "Job finished.\n",
      "Loaded 32561 rows.\n",
      "Starting job f1cc2def-057d-4320-9d19-e0885a024566\n",
      "Job finished.\n",
      "Loaded 16278 rows.\n"
     ]
    }
   ],
   "source": [
    "load_data_into_bigquery(TRAINING_URL, TRAINING_TABLE_ID)\n",
    "load_data_into_bigquery(EVAL_URL, EVAL_TABLE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpUVI8IR2iXH"
   },
   "source": [
    "Confirm that data was imported\n",
    "\n",
    "TODO: replace \\<YOUR PROJECT\\> with your PROJECT_ID\n",
    "\n",
    "Note: --use_bqstorage_api will get data using BigQueryStorage API and will make sure that you are authorized to use it. Make sure that it is enabled for your project: https://cloud.google.com/bigquery/docs/reference/storage/#enabling_the_api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CVy3UkDgx2zi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.01s: 100%|██████████| 1/1 [00:00<00:00, 329.84query/s]                          \n",
      "Downloading: 100%|██████████| 5/5 [00:01<00:00,  3.31rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>297847.0</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>3411.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>74141.0</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>178215.0</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>86958.0</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>176012.0</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age workclass    fnlwgt education  education_num       marital_status  \\\n",
       "0  39.0   Private  297847.0       9th            5.0   Married-civ-spouse   \n",
       "1  72.0   Private   74141.0       9th            5.0   Married-civ-spouse   \n",
       "2  45.0   Private  178215.0       9th            5.0   Married-civ-spouse   \n",
       "3  31.0   Private   86958.0       9th            5.0   Married-civ-spouse   \n",
       "4  55.0   Private  176012.0       9th            5.0   Married-civ-spouse   \n",
       "\n",
       "           occupation relationship                 race   gender  \\\n",
       "0       Other-service         Wife                Black   Female   \n",
       "1     Exec-managerial         Wife   Asian-Pac-Islander   Female   \n",
       "2   Machine-op-inspct         Wife                White   Female   \n",
       "3     Exec-managerial         Wife                White   Female   \n",
       "4        Tech-support         Wife                White   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country income_bracket  \n",
       "0        3411.0           0.0            34.0   United-States          <=50K  \n",
       "1           0.0           0.0            48.0   United-States           >50K  \n",
       "2           0.0           0.0            40.0   United-States           >50K  \n",
       "3           0.0           0.0            40.0   United-States          <=50K  \n",
       "4           0.0           0.0            23.0   United-States          <=50K  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --use_bqstorage_api\n",
    "SELECT * FROM `<YOUR PROJECT>.census_dataset.census_training_table` LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOu-pCksYTtE"
   },
   "source": [
    "## Load census data in TensorFlow DataSet using BigQuery reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_Gm8Mzh62yF"
   },
   "source": [
    "Read and transform cesnus data from BigQuery into TensorFlow DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NgPd9w5m06In"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "  \n",
    "def transform_row(row_dict):\n",
    "  # Trim all string tensors\n",
    "  trimmed_dict = { column:\n",
    "                  (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                  for (column,tensor) in row_dict.items()\n",
    "                  }\n",
    "  # Extract feature column\n",
    "  income_bracket = trimmed_dict.pop('income_bracket')\n",
    "  # Convert feature column to 0.0/1.0\n",
    "  income_bracket_float = tf.cond(tf.equal(tf.strings.strip(income_bracket), '>50K'), \n",
    "                 lambda: tf.constant(1.0), \n",
    "                 lambda: tf.constant(0.0))\n",
    "  return (trimmed_dict, income_bracket_float)\n",
    "\n",
    "def read_bigquery(table_name):\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + PROJECT_ID,\n",
    "      PROJECT_ID, table_name, DATASET_ID,\n",
    "      list(field.name for field in CSV_SCHEMA \n",
    "           if not field.name in UNUSED_COLUMNS),\n",
    "      list(dtypes.double if field.field_type == 'FLOAT64' \n",
    "           else dtypes.string for field in CSV_SCHEMA\n",
    "           if not field.name in UNUSED_COLUMNS),\n",
    "      requested_streams=2)\n",
    "  \n",
    "  # Read the rows in parallel streams\n",
    "  dataset = # TODO -- Your code goes here\n",
    "  # Apply transformation to the dataset\n",
    "  transformed_ds = # TODO -- Your code goes here(transform_row)\n",
    "  return transformed_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4_NlkxZt1rwR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 08:16:55.680866: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-09-10 08:16:55.686447: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200155000 Hz\n",
      "2021-09-10 08:16:55.687680: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5635177c2920 executing computations on platform Host. Devices:\n",
      "2021-09-10 08:16:55.687727: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_io/bigquery/python/ops/bigquery_api.py:214: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "training_ds = read_bigquery(TRAINING_TABLE_ID).shuffle(10000).batch(BATCH_SIZE)\n",
    "eval_ds = read_bigquery(EVAL_TABLE_ID).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_iXxNdSYsDO"
   },
   "source": [
    "## Define feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UClHDwcyhFky"
   },
   "outputs": [],
   "source": [
    "def get_categorical_feature_values(column):\n",
    "  query = 'SELECT DISTINCT TRIM({}) FROM `{}`.{}.{}'.format(column, PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID)\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  dataset_ref = client.dataset(DATASET_ID)\n",
    "  job_config = bigquery.QueryJobConfig()\n",
    "  query_job = client.query(query, job_config=job_config)\n",
    "  result = query_job.to_dataframe()\n",
    "  return result.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h9aAs1ZAtQr-"
   },
   "outputs": [],
   "source": [
    "from tensorflow import feature_column\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['capital_gain', 'capital_loss', 'hours_per_week']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# categorical cols\n",
    "for header in ['workclass', 'marital_status', 'occupation', 'relationship',\n",
    "               'race', 'native_country', 'education']:\n",
    "  categorical_feature = feature_column.categorical_column_with_vocabulary_list(\n",
    "        header, get_categorical_feature_values(header))\n",
    "  categorical_feature_one_hot = feature_column.indicator_column(categorical_feature)\n",
    "  feature_columns.append(categorical_feature_one_hot)\n",
    "\n",
    "# bucketized cols\n",
    "age = feature_column.numeric_column('age')\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfO0bhXXY3GQ"
   },
   "source": [
    "## Build and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnKZKOQX7Qwx"
   },
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Sm-UsB5_zvt0"
   },
   "outputs": [],
   "source": [
    "Dense = tf.keras.layers.Dense\n",
    "model = tf.keras.Sequential(\n",
    "  [\n",
    "    feature_layer,\n",
    "      Dense(100, activation=tf.nn.relu, kernel_initializer='uniform'),\n",
    "      Dense(75, activation=tf.nn.relu),\n",
    "      Dense(50, activation=tf.nn.relu),\n",
    "      Dense(25, activation=tf.nn.relu),\n",
    "      Dense(1, activation=tf.nn.sigmoid)\n",
    "  ])\n",
    "\n",
    "# Compile Keras model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8bSDfQd7T1n"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gPKrlFCN1y00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "Epoch 1/5\n",
      "1018/1018 [==============================] - 13s 13ms/step - loss: 0.5052 - accuracy: 0.8038\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 08:18:01.591898: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018/1018 [==============================] - 10s 9ms/step - loss: 0.3776 - accuracy: 0.8166\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 08:18:11.181031: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018/1018 [==============================] - 9s 9ms/step - loss: 0.3613 - accuracy: 0.8306\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 08:18:20.400916: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018/1018 [==============================] - 9s 9ms/step - loss: 0.3469 - accuracy: 0.8391\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 08:18:29.751204: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018/1018 [==============================] - 9s 9ms/step - loss: 0.3414 - accuracy: 0.8409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 08:18:39.025880: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fed74a28450>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# TODO -- Your code goes here(training_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgKKliXRZG_G"
   },
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgNd5DdU7TEW"
   },
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8eGHVkmI5LBT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 6s 11ms/step - loss: 0.3419 - accuracy: 0.8313\n",
      "Accuracy 0.83130604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 08:18:59.583508: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = # TODO -- Your code goes here(eval_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBIWoGMP7bj1"
   },
   "source": [
    "Evaluate a couple of random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aMou1t1xngXP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6854977],\n",
       "       [0.940263 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = {\n",
    "    'age' : np.array([56, 36]), \n",
    "    'workclass': np.array(['Local-gov', 'Private']), \n",
    "    'education': np.array(['Bachelors', 'Bachelors']), \n",
    "    'marital_status': np.array(['Married-civ-spouse', 'Married-civ-spouse']), \n",
    "    'occupation': np.array(['Tech-support', 'Other-service']), \n",
    "    'relationship': np.array(['Husband', 'Husband']), \n",
    "    'race': np.array(['White', 'Black']), \n",
    "    'gender': np.array(['Male', 'Male']), \n",
    "    'capital_gain': np.array([0, 7298]), \n",
    "    'capital_loss': np.array([0, 0]), \n",
    "    'hours_per_week': np.array([40, 36]), \n",
    "    'native_country': np.array(['United-States', 'United-States'])\n",
    "  }\n",
    "\n",
    "model.predict(sample_x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "bigquery.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
