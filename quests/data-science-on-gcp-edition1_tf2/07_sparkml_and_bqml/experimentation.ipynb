{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "<h1> Experimenting with different models </h1>\n\nIn this notebook, we try out different ideas.  The first thing we have to do is to create a validation set, so that we are not doing experimentation with our independent test dataset."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "BUCKET='cs358-bucket'\n\nimport os\nos.environ['BUCKET'] = BUCKET"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "from __future__ import print_function\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.sql.types import StringType, FloatType, StructType, StructField"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<pyspark.sql.session.SparkSession object at 0x7fce57630e10>\n<SparkContext master=local appName=experimentation>\n"}], "source": "# Create spark session\n\nfrom __future__ import print_function\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\n\nsc = SparkContext('local', 'experimentation')\nspark = SparkSession \\\n    .builder \\\n    .appName(\"experimentation w/ Spark ML\") \\\n    .getOrCreate()\n\nprint(spark)\nprint(sc)"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2> Read dataset </h2>"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "traindays = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv('gs://{}/flights/trainday.csv'.format(BUCKET))\ntraindays.createOrReplaceTempView('traindays')"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "header = 'FL_DATE,UNIQUE_CARRIER,AIRLINE_ID,CARRIER,FL_NUM,ORIGIN_AIRPORT_ID,ORIGIN_AIRPORT_SEQ_ID,ORIGIN_CITY_MARKET_ID,ORIGIN,DEST_AIRPORT_ID,DEST_AIRPORT_SEQ_ID,DEST_CITY_MARKET_ID,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,TAXI_OUT,WHEELS_OFF,WHEELS_ON,TAXI_IN,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,CANCELLED,CANCELLATION_CODE,DIVERTED,DISTANCE,DEP_AIRPORT_LAT,DEP_AIRPORT_LON,DEP_AIRPORT_TZOFFSET,ARR_AIRPORT_LAT,ARR_AIRPORT_LON,ARR_AIRPORT_TZOFFSET,EVENT,NOTIFY_TIME'\n\ndef get_structfield(colname):\n    if colname in ['ARR_DELAY', 'DEP_DELAY', 'DISTANCE', 'TAXI_OUT']:\n        return StructField(colname, FloatType(), True)\n    else:\n        return StructField(colname, StringType(), True)\n\nschema = StructType([get_structfield(colname) for colname in header.split(',')])"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "inputs = 'gs://{}/flights/tzcorr/all_flights-00000-*' # 1/30th\n#inputs = 'gs://{}/flights/tzcorr/all_flights-*'  # FULL\nflights = spark.read\\\n            .schema(schema)\\\n            .csv(inputs.format(BUCKET))\n\n# this view can now be queried ...\nflights.createOrReplaceTempView('flights')"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2> Create separate training and validation data </h2>"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import rand\nSEED = 13\ntraindays = traindays.withColumn(\"holdout\", rand(SEED) > 0.8)  # 80% of data is for training\ntraindays.createOrReplaceTempView('traindays')"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(FL_DATE='2015-01-01', is_train_day='True', holdout=False),\n Row(FL_DATE='2015-01-02', is_train_day='False', holdout=True),\n Row(FL_DATE='2015-01-03', is_train_day='False', holdout=False),\n Row(FL_DATE='2015-01-04', is_train_day='True', holdout=False),\n Row(FL_DATE='2015-01-05', is_train_day='True', holdout=True),\n Row(FL_DATE='2015-01-06', is_train_day='False', holdout=False),\n Row(FL_DATE='2015-01-07', is_train_day='True', holdout=False),\n Row(FL_DATE='2015-01-08', is_train_day='True', holdout=False),\n Row(FL_DATE='2015-01-09', is_train_day='True', holdout=False),\n Row(FL_DATE='2015-01-10', is_train_day='True', holdout=False)]"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "traindays.head(10)"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2> Logistic regression </h2>"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "trainquery = \"\"\"\nSELECT\n  *\nFROM flights f\nJOIN traindays t\nON f.FL_DATE == t.FL_DATE\nWHERE\n  t.is_train_day == 'True' AND\n  t.holdout == False AND\n  f.CANCELLED == '0.00' AND \n  f.DIVERTED == '0.00'\n\"\"\"\ntraindata = spark.sql(trainquery)"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": "Row(FL_DATE='2015-05-16', UNIQUE_CARRIER='VX', AIRLINE_ID='21171', CARRIER='VX', FL_NUM='108', ORIGIN_AIRPORT_ID='12892', ORIGIN_AIRPORT_SEQ_ID='1289203', ORIGIN_CITY_MARKET_ID='32575', ORIGIN='LAX', DEST_AIRPORT_ID='12264', DEST_AIRPORT_SEQ_ID='1226402', DEST_CITY_MARKET_ID='30852', DEST='IAD', CRS_DEP_TIME='2015-05-16T15:25:00', DEP_TIME='2015-05-16T15:20:00', DEP_DELAY=-5.0, TAXI_OUT=12.0, WHEELS_OFF='2015-05-16T15:32:00', WHEELS_ON='2015-05-16T20:07:00', TAXI_IN='7.00', CRS_ARR_TIME='2015-05-16T20:25:00', ARR_TIME='2015-05-16T20:14:00', ARR_DELAY=-11.0, CANCELLED='0.00', CANCELLATION_CODE=None, DIVERTED='0.00', DISTANCE=2288.0, DEP_AIRPORT_LAT='33.94250000', DEP_AIRPORT_LON='-118.40805556', DEP_AIRPORT_TZOFFSET='-25200.0', ARR_AIRPORT_LAT='38.94750000', ARR_AIRPORT_LON='-77.46000000', ARR_AIRPORT_TZOFFSET='-14400.0', EVENT=None, NOTIFY_TIME=None, FL_DATE='2015-05-16', is_train_day='True', holdout=False)"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "traindata.head()"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "def to_example(fields):\n    return LabeledPoint(\\\n              float(fields['ARR_DELAY'] < 15), #ontime \\\n              [ \\\n                  fields['DEP_DELAY'], # DEP_DELAY \\\n                  fields['TAXI_OUT'], # TAXI_OUT \\\n                  fields['DISTANCE'], # DISTANCE \\\n              ])"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "examples = traindata.rdd.map(to_example)"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[-0.16588414918675193,-0.1258966534699923,0.0002796516834817356] 5.1096425611240575\n"}], "source": "lrmodel = LogisticRegressionWithLBFGS.train(examples, intercept=True)\nprint(lrmodel.weights,lrmodel.intercept)"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "lrmodel.setThreshold(0.7) # cancel if prob-of-ontime < 0.7"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2> Evaluate model on the heldout data </h2>\n"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nSELECT\n  *\nFROM flights f\nJOIN traindays t\nON f.FL_DATE == t.FL_DATE\nWHERE\n  t.is_train_day == 'True' AND\n  t.holdout == True AND\n  f.CANCELLED == '0.00' AND \n  f.DIVERTED == '0.00'\n\n"}], "source": "evalquery = trainquery.replace(\"t.holdout == False\",\"t.holdout == True\")\nprint(evalquery)"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "evaldata = spark.sql(evalquery)\nexamples = evaldata.rdd.map(to_example)"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "def eval(labelpred):\n    ''' \n        data = (label, pred)\n            data[0] = label\n            data[1] = pred\n    '''\n    cancel = labelpred.filter(lambda data: data[1] < 0.7)\n    nocancel = labelpred.filter(lambda data: data[1] >= 0.7)\n    corr_cancel = cancel.filter(lambda data: data[0] == int(data[1] >= 0.7)).count()\n    corr_nocancel = nocancel.filter(lambda data: data[0] == int(data[1] >= 0.7)).count()\n    \n    cancel_denom = cancel.count()\n    nocancel_denom = nocancel.count()\n    if cancel_denom == 0:\n        cancel_denom = 1\n    if nocancel_denom == 0:\n        nocancel_denom = 1\n    return {'total_cancel': cancel.count(), \\\n            'correct_cancel': float(corr_cancel)/cancel_denom, \\\n            'total_noncancel': nocancel.count(), \\\n            'correct_noncancel': float(corr_nocancel)/nocancel_denom \\\n           }"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'total_cancel': 6540, 'correct_cancel': 0.7807339449541284, 'total_noncancel': 27572, 'correct_noncancel': 0.9657986362976934}\n"}], "source": "labelpred = examples.map(lambda p: (p.label, lrmodel.predict(p.features)))\nprint(eval(labelpred))"}, {"cell_type": "markdown", "metadata": {}, "source": "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}